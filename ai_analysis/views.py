from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required, user_passes_test
from django.http import JsonResponse, HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib import messages
from django.db.models import Q, Count, Avg
from django.db import models
from django.core.paginator import Paginator
from django.utils import timezone
from datetime import datetime, timedelta
import json
import numpy as np
import pydicom
import os
import threading
import time
import re
import requests
try:
    import onnxruntime as ort
except Exception:
    ort = None
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except Exception:
    AutoTokenizer = None
    AutoModelForSequenceClassification = None

from worklist.models import Study, DicomImage, Series
from accounts.models import User
from .models import (
    AIModel, AIAnalysis, AutoReportTemplate, AutoGeneratedReport,
    AITrainingData, AIPerformanceMetric, AIFeedback, UrgentAlert
)

def is_admin_or_radiologist(user):
    """Check if user is admin or radiologist"""
    return user.is_authenticated and (user.is_admin() or user.is_radiologist())

@login_required
def ai_dashboard(request):
    """AI analysis dashboard with comprehensive overview"""
    user = request.user
    
    # Get AI models statistics
    total_models = AIModel.objects.filter(is_active=True).count()
    active_analyses = AIAnalysis.objects.filter(status__in=['pending', 'processing']).count()
    completed_today = AIAnalysis.objects.filter(
        completed_at__date=timezone.now().date(),
        status='completed'
    ).count()
    
    # Get recent analyses
    if user.is_facility_user():
        recent_analyses = AIAnalysis.objects.filter(
            study__facility=user.facility
        ).select_related('study', 'ai_model').order_by('-requested_at')[:10]
    else:
        recent_analyses = AIAnalysis.objects.select_related(
            'study', 'ai_model'
        ).order_by('-requested_at')[:10]
    
    # Get pending auto-reports
    if user.is_radiologist() or user.is_admin():
        pending_reports = AutoGeneratedReport.objects.filter(
            review_status='pending'
        ).select_related('study', 'ai_analysis').order_by('-generated_at')[:5]
    else:
        pending_reports = []
    
    # Get model performance summary
    model_performance = []
    for model in AIModel.objects.filter(is_active=True)[:5]:
        latest_metric = model.performance_metrics.first()
        model_performance.append({
            'model': model,
            'accuracy': latest_metric.accuracy if latest_metric else 0,
            'total_analyses': model.total_analyses,
            'avg_time': model.avg_processing_time
        })
    
    context = {
        'total_models': total_models,
        'active_analyses': active_analyses,
        'completed_today': completed_today,
        'recent_analyses': recent_analyses,
        'pending_reports': pending_reports,
        'model_performance': model_performance,
        'user': user,
    }
    
    return render(request, 'ai_analysis/dashboard.html', context)

@login_required
@csrf_exempt
def analyze_study(request, study_id):
    """Run AI analysis on study"""
    study = get_object_or_404(Study, id=study_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        # Only administrators or radiologists can initiate new AI analyses
        if not is_admin_or_radiologist(user):
            return JsonResponse({'error': 'Only administrators or radiologists can start AI analyses'}, status=403)
        try:
            # Get selected AI models
            model_ids = request.POST.getlist('ai_models')
            priority = request.POST.get('priority', 'normal')
            
            if not model_ids:
                return JsonResponse({'error': 'Please select at least one AI model'}, status=400)
            
            # Create analyses for each selected model
            analyses = []
            for model_id in model_ids:
                ai_model = get_object_or_404(AIModel, id=model_id, is_active=True)
                
                # Check if analysis already exists
                existing = AIAnalysis.objects.filter(
                    study=study,
                    ai_model=ai_model,
                    status__in=['pending', 'processing', 'completed']
                ).first()
                
                if existing:
                    continue
                
                # Create new analysis
                analysis = AIAnalysis.objects.create(
                    study=study,
                    ai_model=ai_model,
                    priority=priority,
                    status='pending'
                )
                analyses.append(analysis)
            
            # Start processing in background
            if analyses:
                threading.Thread(
                    target=process_ai_analyses,
                    args=(analyses,),
                    daemon=True
                ).start()
            
            return JsonResponse({
                'success': True,
                'message': f'Started AI analysis with {len(analyses)} models',
                'analysis_ids': [a.id for a in analyses]
            })
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    # GET request - show analysis form
    available_models = []
    if is_admin_or_radiologist(user):
        available_models = AIModel.objects.filter(
            is_active=True,
            modality__in=[study.modality.code, 'ALL']
        )
    
    # Get existing analyses
    existing_analyses = AIAnalysis.objects.filter(
        study=study
    ).select_related('ai_model').order_by('-requested_at')
    
    context = {
        'study': study,
        'available_models': available_models,
        'existing_analyses': existing_analyses,
    }
    
    return render(request, 'ai_analysis/analyze_study.html', context)

@login_required
@csrf_exempt
def api_analysis_status(request, analysis_id):
    """Get analysis status and progress"""
    analysis = get_object_or_404(AIAnalysis, id=analysis_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and analysis.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    progress_percentage = 0
    if analysis.status == 'completed':
        progress_percentage = 100
    elif analysis.status == 'processing':
        # Estimate progress based on processing time
        if analysis.started_at:
            elapsed = (timezone.now() - analysis.started_at).total_seconds()
            estimated_total = analysis.ai_model.avg_processing_time or 60
            progress_percentage = min(90, (elapsed / estimated_total) * 100)
    
    # Only administrators/radiologists get full detailed analysis
    clinician = is_admin_or_radiologist(user)
    data = {
        'id': analysis.id,
        'status': analysis.status,
        'progress_percentage': round(progress_percentage, 2),
        'confidence_score': analysis.confidence_score,
        'requested_at': analysis.requested_at.isoformat(),
        'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None,
        'ai_model': {
            'name': analysis.ai_model.name,
            'version': analysis.ai_model.version,
            'type': analysis.ai_model.model_type
        }
    }
    if clinician:
        data.update({
            'findings': analysis.findings,
            'abnormalities_detected': analysis.abnormalities_detected,
            'measurements': analysis.measurements,
            'processing_time': analysis.processing_time,
            'error_message': analysis.error_message,
        })
    else:
        # Minimal, non-intrusive preliminary summary for non-clinician roles
        data.update({
            'summary': 'Preliminary AI review complete' if analysis.status == 'completed' else 'AI review in progress',
        })
    
    return JsonResponse(data)

@login_required
@user_passes_test(is_admin_or_radiologist)
@csrf_exempt
def generate_auto_report(request, study_id):
    """Generate automatic report from AI analysis"""
    study = get_object_or_404(Study, id=study_id)
    user = request.user
    
    # Check facility permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    try:
        # Get completed AI analyses for this study
        analyses = AIAnalysis.objects.filter(
            study=study,
            status='completed'
        ).select_related('ai_model')
        
        if not analyses:
            return JsonResponse({'error': 'No completed AI analyses found for this study'}, status=400)
        
        # Find appropriate template
        template = AutoReportTemplate.objects.filter(
            modality=study.modality.code,
            body_part__icontains=study.body_part,
            is_active=True
        ).first()
        
        if not template:
            # Use generic template
            template = AutoReportTemplate.objects.filter(
                modality=study.modality.code,
                is_active=True
            ).first()
        
        if not template:
            return JsonResponse({'error': 'No suitable report template found'}, status=400)
        
        # Generate report content
        report_data = generate_report_content(study, analyses, template)
        
        # Create auto-generated report
        auto_report = AutoGeneratedReport.objects.create(
            study=study,
            template=template,
            ai_analysis=analyses.first(),  # Primary analysis
            generated_findings=report_data['findings'],
            generated_impression=report_data['impression'],
            generated_recommendations=report_data['recommendations'],
            overall_confidence=report_data['confidence'],
            requires_review=report_data['confidence'] < template.confidence_threshold
        )
        
        return JsonResponse({
            'success': True,
            'report_id': auto_report.id,
            'findings': auto_report.generated_findings,
            'impression': auto_report.generated_impression,
            'recommendations': auto_report.generated_recommendations,
            'confidence': auto_report.overall_confidence,
            'requires_review': auto_report.requires_review
        })
        
    except Exception as e:
        return JsonResponse({'error': f'Error generating report: {str(e)}'}, status=500)

@login_required
@user_passes_test(is_admin_or_radiologist)
def review_auto_report(request, report_id):
    """Review and approve/modify auto-generated report"""
    auto_report = get_object_or_404(AutoGeneratedReport, id=report_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and auto_report.study.facility != user.facility:
        messages.error(request, 'Permission denied')
        return redirect('ai_analysis:ai_dashboard')
    
    if request.method == 'POST':
        action = request.POST.get('action')
        
        if action == 'approve':
            auto_report.review_status = 'approved'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            # Create final report (when reports app is available)
            # auto_report.approve_and_create_report(user)
            
            messages.success(request, 'Auto-generated report approved successfully')
            
        elif action == 'modify':
            auto_report.generated_findings = request.POST.get('findings')
            auto_report.generated_impression = request.POST.get('impression')
            auto_report.generated_recommendations = request.POST.get('recommendations')
            auto_report.review_status = 'modified'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            messages.success(request, 'Auto-generated report modified and approved')
            
        elif action == 'reject':
            auto_report.review_status = 'rejected'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            messages.success(request, 'Auto-generated report rejected')
        
        return redirect('ai_analysis:ai_dashboard')
    
    # GET request - show review form
    context = {
        'auto_report': auto_report,
        'study': auto_report.study,
        'ai_analysis': auto_report.ai_analysis,
    }
    
    return render(request, 'ai_analysis/review_auto_report.html', context)

@login_required
@csrf_exempt
def api_ai_feedback(request, analysis_id):
    """Submit feedback on AI analysis"""
    analysis = get_object_or_404(AIAnalysis, id=analysis_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and analysis.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            feedback = AIFeedback.objects.create(
                ai_analysis=analysis,
                user=user,
                feedback_type=data.get('feedback_type'),
                rating=data.get('rating'),
                comments=data.get('comments', ''),
                incorrect_findings=data.get('incorrect_findings', []),
                missed_findings=data.get('missed_findings', []),
                suggestions=data.get('suggestions', '')
            )
            
            return JsonResponse({
                'success': True,
                'feedback_id': feedback.id,
                'message': 'Feedback submitted successfully'
            })
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

@login_required
@user_passes_test(lambda u: u.is_admin())
def model_management(request):
    """AI model management interface"""
    models = AIModel.objects.all().order_by('-created_at')
    
    # Search and filtering
    search_query = request.GET.get('search', '')
    if search_query:
        models = models.filter(
            Q(name__icontains=search_query) |
            Q(description__icontains=search_query) |
            Q(modality__icontains=search_query)
        )
    
    model_type_filter = request.GET.get('model_type', '')
    if model_type_filter:
        models = models.filter(model_type=model_type_filter)
    
    # Pagination
    paginator = Paginator(models, 20)
    page_number = request.GET.get('page')
    models_page = paginator.get_page(page_number)
    
    context = {
        'models': models_page,
        'search_query': search_query,
        'model_type_filter': model_type_filter,
        'model_types': AIModel.MODEL_TYPES,
    }
    
    return render(request, 'ai_analysis/model_management.html', context)

@login_required
@csrf_exempt
def api_realtime_analyses(request):
    """Get real-time AI analysis updates"""
    user = request.user
    
    # Get timestamp from request
    last_update = request.GET.get('last_update')
    
    try:
        if last_update:
            last_update_time = timezone.datetime.fromisoformat(last_update.replace('Z', '+00:00'))
        else:
            last_update_time = timezone.now() - timezone.timedelta(minutes=5)
    except:
        last_update_time = timezone.now() - timezone.timedelta(minutes=5)
    
    # Get analyses updated since last check
    if user.is_facility_user():
        analyses = AIAnalysis.objects.filter(
            study__facility=user.facility,
            requested_at__gt=last_update_time
        ).select_related('study', 'ai_model').order_by('-requested_at')[:20]
    else:
        analyses = AIAnalysis.objects.filter(
            requested_at__gt=last_update_time
        ).select_related('study', 'ai_model').order_by('-requested_at')[:20]
    
    clinician = is_admin_or_radiologist(user)
    analyses_data = []
    for analysis in analyses:
        item = {
            'id': analysis.id,
            'study_id': analysis.study.id,
            'accession_number': analysis.study.accession_number,
            'patient_name': analysis.study.patient.full_name,
            'ai_model': analysis.ai_model.name,
            'status': analysis.status,
            'priority': analysis.priority,
            'requested_at': analysis.requested_at.isoformat(),
            'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None,
        }
        if clinician:
            item['confidence_score'] = analysis.confidence_score
        analyses_data.append(item)
    
    return JsonResponse({
        'analyses': analyses_data,
        'timestamp': timezone.now().isoformat(),
        'count': len(analyses_data)
    })

@login_required
@user_passes_test(lambda u: u.is_admin())
def ai_reporting_dashboard(request):
    """Comprehensive AI reporting dashboard"""
    # Get date range from request
    end_date = timezone.now().date()
    start_date = end_date - timedelta(days=30)
    
    date_filter = request.GET.get('date_range', '30d')
    if date_filter == '7d':
        start_date = end_date - timedelta(days=7)
    elif date_filter == '90d':
        start_date = end_date - timedelta(days=90)
    elif date_filter == '1y':
        start_date = end_date - timedelta(days=365)
    
    # Overall statistics
    total_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date
    ).count()
    
    completed_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='completed'
    ).count()
    
    failed_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='failed'
    ).count()
    
    # Success rate
    success_rate = (completed_analyses / total_analyses * 100) if total_analyses > 0 else 0
    
    # Average processing time
    avg_processing_time = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='completed',
        processing_time__isnull=False
    ).aggregate(Avg('processing_time'))['processing_time__avg'] or 0
    
    # Model performance breakdown
    model_performance = []
    for model in AIModel.objects.filter(is_active=True):
        model_analyses = AIAnalysis.objects.filter(
            ai_model=model,
            requested_at__date__gte=start_date,
            requested_at__date__lte=end_date
        )
        
        total = model_analyses.count()
        completed = model_analyses.filter(status='completed').count()
        failed = model_analyses.filter(status='failed').count()
        avg_confidence = model_analyses.filter(
            status='completed',
            confidence_score__isnull=False
        ).aggregate(Avg('confidence_score'))['confidence_score__avg'] or 0
        
        model_performance.append({
            'model': model,
            'total_analyses': total,
            'completed': completed,
            'failed': failed,
            'success_rate': (completed / total * 100) if total > 0 else 0,
            'avg_confidence': round(avg_confidence, 3),
            'avg_processing_time': model.avg_processing_time or 0
        })
    
    # Auto-report statistics
    total_auto_reports = AutoGeneratedReport.objects.filter(
        generated_at__date__gte=start_date,
        generated_at__date__lte=end_date
    ).count()
    
    approved_reports = AutoGeneratedReport.objects.filter(
        generated_at__date__gte=start_date,
        generated_at__date__lte=end_date,
        review_status='approved'
    ).count()
    
    pending_reports = AutoGeneratedReport.objects.filter(
        review_status='pending'
    ).count()
    
    # Daily analysis trends
    daily_trends = []
    current_date = start_date
    while current_date <= end_date:
        daily_count = AIAnalysis.objects.filter(
            requested_at__date=current_date
        ).count()
        daily_trends.append({
            'date': current_date.isoformat(),
            'count': daily_count
        })
        current_date += timedelta(days=1)
    
    # Top performing models
    top_models = sorted(model_performance, key=lambda x: x['success_rate'], reverse=True)[:5]
    
    # Recent feedback
    recent_feedback = AIFeedback.objects.filter(
        created_at__date__gte=start_date
    ).select_related('ai_analysis__ai_model', 'user').order_by('-created_at')[:10]
    
    context = {
        'total_analyses': total_analyses,
        'completed_analyses': completed_analyses,
        'failed_analyses': failed_analyses,
        'success_rate': round(success_rate, 2),
        'avg_processing_time': round(avg_processing_time, 2),
        'model_performance': model_performance,
        'total_auto_reports': total_auto_reports,
        'approved_reports': approved_reports,
        'pending_reports': pending_reports,
        'daily_trends': daily_trends,
        'top_models': top_models,
        'recent_feedback': recent_feedback,
        'date_filter': date_filter,
        'start_date': start_date,
        'end_date': end_date,
    }
    
    return render(request, 'ai_analysis/reporting_dashboard.html', context)

@login_required
@user_passes_test(lambda u: u.is_admin())
def verify_ai_models(request):
    """Verify that all AI models are working correctly"""
    verification_results = []
    
    # Get all active AI models
    models = AIModel.objects.filter(is_active=True)
    
    for model in models:
        result = {
            'model': model,
            'status': 'unknown',
            'last_test': None,
            'test_results': {},
            'issues': []
        }
        
        # Check if model files exist
        if not os.path.exists(model.model_file_path):
            result['issues'].append('Model file not found')
            result['status'] = 'error'
        
        # Check recent analyses
        recent_analyses = AIAnalysis.objects.filter(
            ai_model=model,
            requested_at__gte=timezone.now() - timedelta(days=7)
        )
        
        if recent_analyses.exists():
            completed = recent_analyses.filter(status='completed').count()
            failed = recent_analyses.filter(status='failed').count()
            total = recent_analyses.count()
            
            success_rate = (completed / total * 100) if total > 0 else 0
            
            result['test_results'] = {
                'total_tests': total,
                'completed': completed,
                'failed': failed,
                'success_rate': round(success_rate, 2)
            }
            
            if success_rate >= 95:
                result['status'] = 'excellent'
            elif success_rate >= 85:
                result['status'] = 'good'
            elif success_rate >= 70:
                result['status'] = 'warning'
            else:
                result['status'] = 'error'
                result['issues'].append(f'Low success rate: {success_rate:.1f}%')
        else:
            result['status'] = 'untested'
            result['issues'].append('No recent test data available')
        
        # Check performance metrics
        latest_metric = model.performance_metrics.first()
        if latest_metric:
            result['last_test'] = latest_metric.evaluation_date
            if latest_metric.accuracy < 0.8:
                result['issues'].append(f'Low accuracy: {latest_metric.accuracy:.3f}')
        else:
            result['issues'].append('No performance metrics available')
        
        verification_results.append(result)
    
    # Overall system status
    statuses = [r['status'] for r in verification_results]
    if 'error' in statuses:
        overall_status = 'error'
    elif 'warning' in statuses:
        overall_status = 'warning'
    elif 'untested' in statuses:
        overall_status = 'warning'
    else:
        overall_status = 'good'
    
    context = {
        'verification_results': verification_results,
        'overall_status': overall_status,
        'total_models': len(verification_results),
        'error_count': statuses.count('error'),
        'warning_count': statuses.count('warning') + statuses.count('untested'),
        'good_count': statuses.count('good') + statuses.count('excellent'),
    }
    
    return render(request, 'ai_analysis/model_verification.html', context)

@login_required
@user_passes_test(lambda u: u.is_admin())
@csrf_exempt
def run_model_test(request, model_id):
    """Run a test on a specific AI model"""
    model = get_object_or_404(AIModel, id=model_id)
    
    if request.method == 'POST':
        try:
            # Get a test study for this modality
            test_study = Study.objects.filter(
                modality__code=model.modality
            ).first()
            
            if not test_study:
                return JsonResponse({
                    'success': False,
                    'error': f'No test studies available for modality {model.modality}'
                })
            
            # Create a test analysis
            test_analysis = AIAnalysis.objects.create(
                study=test_study,
                ai_model=model,
                priority='high',
                status='pending'
            )
            
            # Run the analysis in background
            threading.Thread(
                target=process_ai_analyses,
                args=([test_analysis],),
                daemon=True
            ).start()
            
            return JsonResponse({
                'success': True,
                'analysis_id': test_analysis.id,
                'message': f'Test started for {model.name}'
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            })
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

def process_ai_analyses(analyses):
    """Background task to process AI analyses"""
    for analysis in analyses:
        try:
            analysis.start_processing()
            
            # Simulate AI processing (replace with actual AI model inference)
            results = simulate_ai_analysis(analysis)
            
            # Complete the analysis
            analysis.complete_analysis(results)
            
            # Update model statistics
            model = analysis.ai_model
            model.total_analyses += 1
            if analysis.processing_time:
                # Update average processing time
                if model.avg_processing_time > 0:
                    model.avg_processing_time = (
                        model.avg_processing_time + analysis.processing_time
                    ) / 2
                else:
                    model.avg_processing_time = analysis.processing_time
            model.save()
            
        except Exception as e:
            analysis.status = 'failed'
            analysis.error_message = str(e)
            analysis.save()

def simulate_ai_analysis(analysis):
    """Enhanced AI simulation with severity grading and realistic findings."""
    modality = analysis.study.modality.code
    study = analysis.study
    
    # Base confidence calculation
    confidence = 0.85
    if AutoTokenizer and AutoModelForSequenceClassification:
        try:
            # Lightweight sentiment-like proxy to modulate confidence
            model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            text = f"Preliminary {modality} analysis"
            inputs = tokenizer(text, return_tensors='pt')
            outputs = model(**inputs)
            scores = outputs.logits.softmax(dim=-1).detach().numpy()[0]
            confidence = float(scores.max()) * 0.2 + 0.8  # keep range ~0.8-1.0
        except Exception:
            confidence = 0.88
    
    # Simulate processing time
    time.sleep(2)
    
    # Simulate different severity levels based on clinical info and random factors
    import random
    severity_factor = random.random()
    
    # Check clinical info for urgent keywords
    clinical_info = (study.clinical_info or '').lower()
    urgent_keywords = ['chest pain', 'stroke', 'trauma', 'emergency', 'urgent', 'acute', 'bleeding', 'fracture']
    has_urgent_keywords = any(keyword in clinical_info for keyword in urgent_keywords)
    
    # Determine severity grade
    if has_urgent_keywords and severity_factor > 0.8:
        severity_grade = 'critical'
        severity_score = 0.9 + (severity_factor * 0.1)
    elif has_urgent_keywords or severity_factor > 0.7:
        severity_grade = 'severe'
        severity_score = 0.7 + (severity_factor * 0.2)
    elif severity_factor > 0.5:
        severity_grade = 'moderate'
        severity_score = 0.4 + (severity_factor * 0.3)
    elif severity_factor > 0.3:
        severity_grade = 'mild'
        severity_score = 0.1 + (severity_factor * 0.3)
    else:
        severity_grade = 'normal'
        severity_score = 0.0 + (severity_factor * 0.1)
    
    # Generate findings based on modality and severity
    findings = ""
    abnormalities = []
    measurements = {}
    urgent_findings = []
    
    if modality == 'CT':
        if severity_grade == 'critical':
            findings = "CRITICAL: Large acute intracranial hemorrhage identified. Significant mass effect with midline shift."
            abnormalities = ["Large intracranial hemorrhage", "Mass effect", "Midline shift"]
            measurements = {"hemorrhage_volume": "45 mL", "midline_shift": "8 mm"}
            urgent_findings = ["Large intracranial hemorrhage requiring immediate neurosurgical consultation"]
        elif severity_grade == 'severe':
            findings = "Acute ischemic stroke in the middle cerebral artery territory. Early signs of infarction."
            abnormalities = ["Acute ischemic stroke", "MCA territory involvement"]
            measurements = {"infarct_volume": "15 mL", "ASPECTS_score": "7"}
            urgent_findings = ["Acute stroke - consider thrombolysis"]
        elif severity_grade == 'moderate':
            findings = "Small chronic lacunar infarcts. Mild cerebral atrophy."
            abnormalities = ["Chronic lacunar infarcts", "Mild cerebral atrophy"]
            measurements = {"lesion_count": 3, "brain_volume": "1380 mL"}
        elif severity_grade == 'mild':
            findings = "Minimal white matter changes consistent with small vessel disease."
            abnormalities = ["Minimal white matter changes"]
            measurements = {"white_matter_score": "Fazekas 1"}
        else:
            findings = "No acute intracranial abnormality. Brain parenchyma appears normal."
            measurements = {"brain_volume": "1450 mL", "ventricle_size": "normal"}
            
    elif modality == 'MR':
        if severity_grade == 'critical':
            findings = "CRITICAL: Large acute stroke with hemorrhagic transformation. Significant edema."
            abnormalities = ["Acute stroke with hemorrhage", "Cerebral edema"]
            measurements = {"lesion_volume": "60 mL", "edema_extent": "severe"}
            urgent_findings = ["Hemorrhagic transformation of stroke - immediate management required"]
        elif severity_grade == 'severe':
            findings = "Multiple acute demyelinating lesions consistent with active MS. Enhancement present."
            abnormalities = ["Multiple demyelinating lesions", "Active inflammation"]
            measurements = {"lesion_count": 12, "enhancing_lesions": 4}
            urgent_findings = ["Active demyelination - consider high-dose steroids"]
        elif severity_grade == 'moderate':
            findings = "Several T2 hyperintense lesions in periventricular white matter."
            abnormalities = ["T2 hyperintense lesions"]
            measurements = {"lesion_count": 5, "largest_lesion": "8 mm"}
        elif severity_grade == 'mild':
            findings = "Few small T2 hyperintensities, likely related to small vessel disease."
            abnormalities = ["Small T2 hyperintensities"]
            measurements = {"lesion_count": 2}
        else:
            findings = "Normal brain MRI. No evidence of acute infarction or hemorrhage."
            measurements = {"lesion_count": 0, "white_matter": "normal"}
            
    elif modality == 'XR':
        if severity_grade == 'critical':
            findings = "CRITICAL: Large pneumothorax with mediastinal shift. Immediate decompression needed."
            abnormalities = ["Large pneumothorax", "Mediastinal shift"]
            measurements = {"pneumothorax_size": "75%", "mediastinal_shift": "4 cm"}
            urgent_findings = ["Tension pneumothorax - immediate chest tube required"]
        elif severity_grade == 'severe':
            findings = "Extensive bilateral pneumonia with consolidation. Possible ARDS."
            abnormalities = ["Bilateral pneumonia", "Consolidation"]
            measurements = {"affected_lung": "80%", "consolidation": "bilateral"}
            urgent_findings = ["Severe pneumonia - consider ICU management"]
        elif severity_grade == 'moderate':
            findings = "Right lower lobe pneumonia. Pleural effusion present."
            abnormalities = ["Right lower lobe pneumonia", "Pleural effusion"]
            measurements = {"consolidation_size": "6 cm", "effusion_volume": "moderate"}
        elif severity_grade == 'mild':
            findings = "Minimal bibasilar atelectasis. Heart size normal."
            abnormalities = ["Minimal atelectasis"]
            measurements = {"heart_size": "normal", "lung_fields": "mostly clear"}
        else:
            findings = "Chest X-ray shows clear lungs. Heart size is normal."
            measurements = {"heart_size": "normal", "lung_fields": "clear"}
    else:
        if severity_grade in ['critical', 'severe']:
            findings = f"URGENT: {modality} study shows significant abnormalities requiring immediate attention."
            abnormalities = ["Significant abnormalities detected"]
            urgent_findings = ["Urgent radiologist review required"]
        else:
            findings = "Study reviewed by AI. No acute abnormalities detected."
            abnormalities = []
            measurements = {}
    
    return {
        'findings': findings,
        'abnormalities': abnormalities,
        'confidence': confidence,
        'measurements': measurements,
        'severity_grade': severity_grade,
        'severity_score': severity_score,
        'urgent_findings': urgent_findings
    }

def generate_report_content(study, analyses, template):
    """Generate comprehensive report content from AI analyses"""
    findings = []
    technical_findings = []
    quality_metrics = []
    hu_analysis = []
    image_statistics = []
    recommendations = []
    
    overall_confidence = 0.0
    confidence_count = 0
    
    # Process each analysis result
    for analysis in analyses:
        if analysis.results and analysis.status == 'completed':
            results = analysis.results
            analysis_type = results.get('analysis_type', 'unknown')
            
            # Add confidence to overall calculation
            if 'confidence' in results:
                overall_confidence += results['confidence']
                confidence_count += 1
            
            # Process different types of analyses
            if analysis_type == 'metadata_analysis':
                technical_findings.extend(results.get('findings', []))
                if 'technical_parameters' in results:
                    tech_params = results['technical_parameters']
                    technical_findings.append(f"Study Date: {tech_params.get('study_date', 'Unknown')}")
                    technical_findings.append(f"Modality: {tech_params.get('modality', 'Unknown')}")
                    if tech_params.get('body_part'):
                        technical_findings.append(f"Body Part: {tech_params['body_part']}")
            
            elif analysis_type == 'image_statistics':
                image_statistics.extend(results.get('findings', []))
                if 'statistics' in results:
                    stats = results['statistics']
                    image_statistics.append(f"Images analyzed: {stats.get('images_analyzed', 0)}")
                    image_statistics.append(f"Mean intensity: {stats.get('mean_intensity', 0):.1f}")
                    image_statistics.append(f"Intensity range: {stats.get('min_value', 0):.0f} - {stats.get('max_value', 0):.0f}")
            
            elif analysis_type == 'hounsfield_analysis':
                hu_analysis.extend(results.get('findings', []))
                if 'hu_statistics' in results:
                    hu_stats = results['hu_statistics']
                    hu_analysis.append(f"HU range: {hu_stats.get('min_hu', 0):.0f} to {hu_stats.get('max_hu', 0):.0f}")
                    hu_analysis.append(f"Mean HU: {hu_stats.get('mean_hu', 0):.1f}")
                
                if 'calibration_check' in results and results['calibration_check']:
                    cal_check = results['calibration_check']
                    if 'water_hu_deviation' in cal_check:
                        deviation = cal_check['water_hu_deviation']
                        if deviation > 5:
                            hu_analysis.append(f"⚠️ Water HU calibration deviation: {deviation:.1f} HU")
                        else:
                            hu_analysis.append(f"✓ Water HU calibration within acceptable range")
            
            elif analysis_type == 'report_generation':
                if 'clinical_observations' in results:
                    findings.extend(results['clinical_observations'])
                if 'recommendations' in results:
                    recommendations.extend(results['recommendations'])
    
    # Calculate overall confidence
    if confidence_count > 0:
        overall_confidence = overall_confidence / confidence_count
    else:
        overall_confidence = 0.5
    
    # Build comprehensive findings using template format
    template_vars = {
        'clinical_info': study.clinical_info or 'Not provided',
        'study_date': study.study_date.strftime('%Y-%m-%d') if study.study_date else 'Unknown',
        'study_time': study.study_date.strftime('%H:%M:%S') if study.study_date else 'Unknown',
        'modality': study.modality.code,
        'body_part': study.body_part or 'Not specified',
        'technical_findings': '\n'.join([f"• {tf}" for tf in technical_findings]) or 'Standard technique',
        'quality_metrics': '\n'.join([f"• {qm}" for qm in image_statistics]) or 'Image quality adequate',
        'hu_analysis': '\n'.join([f"• {hu}" for hu in hu_analysis]) or 'Not applicable',
        'image_statistics': '\n'.join([f"• {stat}" for stat in image_statistics]) or 'Not analyzed',
        'ai_findings': '\n'.join([f"• {finding}" for finding in findings]) or 'No specific findings noted',
        'ai_impression': f"Automated analysis completed with {overall_confidence:.0%} confidence",
        'ai_recommendations': '\n'.join([f"• {rec}" for rec in recommendations]) or '• Clinical correlation recommended\n• Radiologist review required',
        'quality_assessment': f"Overall analysis confidence: {overall_confidence:.1%}"
    }
    
    # Format the template content
    try:
        formatted_findings = template.template_content.format(**template_vars)
    except (AttributeError, KeyError):
        # Fallback if template formatting fails
        formatted_findings = f"""AUTOMATED ANALYSIS REPORT

STUDY INFORMATION:
{template_vars['ai_findings']}

TECHNICAL ASSESSMENT:
{template_vars['technical_findings']}

QUALITY METRICS:
{template_vars['quality_metrics']}

RECOMMENDATIONS:
{template_vars['ai_recommendations']}

CONFIDENCE: {overall_confidence:.1%}
"""
    
    return {
        'findings': formatted_findings,
        'impression': template_vars['ai_impression'],
        'recommendations': template_vars['ai_recommendations'],
        'confidence': overall_confidence,
        'technical_findings': template_vars['technical_findings'],
        'quality_metrics': template_vars['quality_metrics'],
        'hu_analysis': template_vars['hu_analysis'],
        'image_statistics': template_vars['image_statistics'],
        'quality_assessment': template_vars['quality_assessment']
    }


@login_required
@user_passes_test(is_admin_or_radiologist)
def urgent_alerts_dashboard(request):
    """Dashboard for managing urgent alerts"""
    user = request.user
    
    # Get urgent alerts based on user permissions
    if user.is_facility_user():
        alerts = UrgentAlert.objects.filter(
            study__facility=user.facility
        ).select_related('study', 'ai_analysis', 'acknowledged_by', 'resolved_by').order_by('-created_at')
    else:
        alerts = UrgentAlert.objects.select_related(
            'study', 'ai_analysis', 'acknowledged_by', 'resolved_by'
        ).order_by('-created_at')
    
    # Filter by status
    status_filter = request.GET.get('status', 'pending')
    if status_filter and status_filter != 'all':
        alerts = alerts.filter(status=status_filter)
    
    # Statistics
    stats = {
        'total_alerts': alerts.count(),
        'pending': alerts.filter(status='pending').count(),
        'acknowledged': alerts.filter(status='acknowledged').count(),
        'resolved': alerts.filter(status='resolved').count(),
        'avg_response_time': alerts.filter(
            response_time_minutes__isnull=False
        ).aggregate(avg_time=models.Avg('response_time_minutes'))['avg_time'] or 0
    }
    
    # Pagination
    from django.core.paginator import Paginator
    paginator = Paginator(alerts, 20)
    page_number = request.GET.get('page')
    alerts_page = paginator.get_page(page_number)
    
    context = {
        'alerts': alerts_page,
        'stats': stats,
        'status_filter': status_filter,
        'user': user,
    }
    
    return render(request, 'ai_analysis/urgent_alerts_dashboard.html', context)

@login_required
@user_passes_test(is_admin_or_radiologist)
def urgent_alert_detail(request, alert_id):
    """View and manage a specific urgent alert"""
    from .models import UrgentAlert
    
    alert = get_object_or_404(UrgentAlert, id=alert_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and alert.study.facility != user.facility:
        messages.error(request, 'Permission denied')
        return redirect('ai_analysis:urgent_alerts_dashboard')
    
    if request.method == 'POST':
        action = request.POST.get('action')
        
        if action == 'acknowledge':
            alert.acknowledge(user)
            messages.success(request, 'Alert acknowledged successfully')
            
        elif action == 'resolve':
            resolution_notes = request.POST.get('resolution_notes', '')
            alert.resolve(user, resolution_notes)
            messages.success(request, 'Alert resolved successfully')
            
        elif action == 'escalate':
            escalate_to_id = request.POST.get('escalate_to')
            if escalate_to_id:
                escalate_to_user = get_object_or_404(User, id=escalate_to_id)
                alert.escalate(escalate_to_user)
                messages.success(request, f'Alert escalated to {escalate_to_user.get_full_name()}')
        
        return redirect('ai_analysis:urgent_alert_detail', alert_id=alert.id)
    
    # Get potential escalation targets (other radiologists)
    escalation_targets = User.objects.filter(
        role='radiologist',
        is_active=True
    ).exclude(id=user.id)
    
    context = {
        'alert': alert,
        'study': alert.study,
        'ai_analysis': alert.ai_analysis,
        'escalation_targets': escalation_targets,
    }
    
    return render(request, 'ai_analysis/urgent_alert_detail.html', context)

@login_required
@csrf_exempt
def api_urgent_alert_status(request, alert_id):
    """API endpoint for urgent alert status updates"""
    from .models import UrgentAlert
    
    alert = get_object_or_404(UrgentAlert, id=alert_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and alert.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            action = data.get('action')
            
            if action == 'acknowledge' and not alert.acknowledged_by:
                alert.acknowledge(user)
                return JsonResponse({
                    'success': True,
                    'message': 'Alert acknowledged',
                    'acknowledged_by': user.get_full_name(),
                    'acknowledged_at': alert.acknowledged_at.isoformat()
                })
            
            elif action == 'resolve':
                resolution_notes = data.get('resolution_notes', '')
                alert.resolve(user, resolution_notes)
                return JsonResponse({
                    'success': True,
                    'message': 'Alert resolved',
                    'resolved_by': user.get_full_name(),
                    'resolved_at': alert.resolved_at.isoformat()
                })
            
            return JsonResponse({'error': 'Invalid action or alert already processed'}, status=400)
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    # GET request - return alert status
    return JsonResponse({
        'id': alert.id,
        'status': alert.status,
        'alert_type': alert.alert_type,
        'title': alert.title,
        'description': alert.description,
        'severity_score': alert.severity_score,
        'critical_findings': alert.critical_findings,
        'acknowledged_by': alert.acknowledged_by.get_full_name() if alert.acknowledged_by else None,
        'acknowledged_at': alert.acknowledged_at.isoformat() if alert.acknowledged_at else None,
        'resolved_by': alert.resolved_by.get_full_name() if alert.resolved_by else None,
        'resolved_at': alert.resolved_at.isoformat() if alert.resolved_at else None,
        'response_time_minutes': alert.response_time_minutes,
        'created_at': alert.created_at.isoformat(),
        'study': {
            'id': alert.study.id,
            'accession_number': alert.study.accession_number,
            'patient_name': alert.study.patient.full_name,
            'modality': alert.study.modality.code,
            'study_description': alert.study.study_description
        }
    })

@login_required
@csrf_exempt
def api_medical_references(request):
    """
    Provide curated external references based on query keywords (e.g., suspected finding/body part/modality).
    Returns titles and URLs from public reputable sources where possible.
    """
    try:
        query = request.GET.get('q', '').strip()
        if not query:
            return JsonResponse({'success': False, 'error': 'Missing query parameter q'}, status=400)

        # Basic source filters (avoid scraping TOS-sensitive sites; use public pages where permissible)
        sources = [
            'site:radiopaedia.org',
            'site:nih.gov',
            'site:ncbi.nlm.nih.gov',
            'site:who.int',
            'site:rsna.org'
        ]
        search_query = f"{query} ({' OR '.join(sources)})"

        # Use a simple web search via DuckDuckGo HTML (no API key); degrade gracefully if blocked
        ddg_url = 'https://duckduckgo.com/html/'
        params = { 'q': search_query }
        headers = { 'User-Agent': 'Mozilla/5.0 (compatible; NoctisPro/1.0)' }
        results = []
        try:
            resp = requests.post(ddg_url, data=params, headers=headers, timeout=8)
            if resp.ok:
                html = resp.text
                # Parse minimal anchors
                for m in re.finditer(r'<a[^>]+class="result__a"[^>]*href="([^"]+)"[^>]*>(.*?)</a>', html, re.I | re.S):
                    url = m.group(1)
                    title_raw = re.sub('<[^<]+?>', '', m.group(2))
                    title = re.sub(r'\s+', ' ', title_raw).strip()
                    if url and title:
                        results.append({'title': title, 'url': url})
                    if len(results) >= 8:
                        break
        except Exception:
            # If search unavailable, return empty list
            results = []

        return JsonResponse({
            'success': True,
            'query': query,
            'references': results
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)
