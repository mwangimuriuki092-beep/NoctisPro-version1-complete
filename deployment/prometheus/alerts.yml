# Prometheus Alert Rules for NoctisPro PACS

groups:
  # ============================================================================
  # System Resource Alerts
  # ============================================================================
  - name: system_alerts
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High CPU usage detected on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current value: {{ $value }}%)"
      
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "Critical CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 95% (current value: {{ $value }}%)"
      
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current value: {{ $value }}%)"
      
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk usage is above 80% on {{ $labels.mountpoint }} (current value: {{ $value }}%)"
      
      - alert: DiskSpaceCritical
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Disk usage is above 90% on {{ $labels.mountpoint }} (current value: {{ $value }}%)"

  # ============================================================================
  # PostgreSQL Database Alerts
  # ============================================================================
  - name: postgres_alerts
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding"
      
      - alert: PostgreSQLTooManyConnections
        expr: sum by (instance) (pg_stat_activity_count) > 80
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Too many PostgreSQL connections"
          description: "PostgreSQL has {{ $value }} active connections (threshold: 80)"
      
      - alert: PostgreSQLSlowQueries
        expr: avg by (datname) (rate(pg_stat_statements_mean_time_seconds[5m])) > 1
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow queries detected in PostgreSQL"
          description: "Average query time is {{ $value }}s in database {{ $labels.datname }}"
      
      - alert: PostgreSQLDeadlocks
        expr: rate(pg_stat_database_deadlocks[5m]) > 0
        for: 1m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database deadlocks detected"
          description: "{{ $value }} deadlocks per second in database {{ $labels.datname }}"
      
      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag > 30
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "PostgreSQL replication lag"
          description: "Replication lag is {{ $value }} seconds"

  # ============================================================================
  # Redis Cache Alerts
  # ============================================================================
  - name: redis_alerts
    interval: 30s
    rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          component: cache
        annotations:
          summary: "Redis is down"
          description: "Redis instance {{ $labels.instance }} is not responding"
      
      - alert: RedisHighMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis memory usage is {{ $value }}%"
      
      - alert: RedisTooManyConnections
        expr: redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Too many Redis connections"
          description: "Redis has {{ $value }} connected clients"
      
      - alert: RedisKeyEvictionRate
        expr: rate(redis_evicted_keys_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "High Redis key eviction rate"
          description: "Redis is evicting {{ $value }} keys per second"

  # ============================================================================
  # Django Application Alerts
  # ============================================================================
  - name: django_alerts
    interval: 30s
    rules:
      - alert: DjangoDown
        expr: up{job="noctis-web"} == 0
        for: 1m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "Django application is down"
          description: "Django web service is not responding"
      
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High response time"
          description: "95th percentile response time is {{ $value }}s"
      
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

  # ============================================================================
  # Celery Worker Alerts
  # ============================================================================
  - name: celery_alerts
    interval: 30s
    rules:
      - alert: CeleryWorkersDown
        expr: celery_workers_active == 0
        for: 2m
        labels:
          severity: critical
          component: celery
        annotations:
          summary: "No active Celery workers"
          description: "All Celery workers are down"
      
      - alert: CeleryQueueBacklog
        expr: celery_queue_length > 1000
        for: 10m
        labels:
          severity: warning
          component: celery
        annotations:
          summary: "Large Celery queue backlog"
          description: "Queue {{ $labels.queue }} has {{ $value }} pending tasks"
      
      - alert: CeleryTaskFailureRate
        expr: rate(celery_task_failed_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: celery
        annotations:
          summary: "High Celery task failure rate"
          description: "Task failure rate is {{ $value }} per second"

  # ============================================================================
  # DICOM Service Alerts
  # ============================================================================
  - name: dicom_alerts
    interval: 30s
    rules:
      - alert: DICOMServiceDown
        expr: up{job="dicom-scp"} == 0
        for: 2m
        labels:
          severity: critical
          component: dicom
        annotations:
          summary: "DICOM SCP service is down"
          description: "DICOM receiver is not responding"
      
      - alert: HighDICOMProcessingTime
        expr: histogram_quantile(0.95, rate(dicom_processing_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
          component: dicom
        annotations:
          summary: "DICOM processing is slow"
          description: "95th percentile processing time is {{ $value }}s"
      
      - alert: DICOMStorageSpaceLow
        expr: (dicom_storage_used_bytes / dicom_storage_total_bytes) * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: dicom
        annotations:
          summary: "DICOM storage space is low"
          description: "DICOM storage is {{ $value }}% full"

  # ============================================================================
  # Backup Alerts
  # ============================================================================
  - name: backup_alerts
    interval: 60s
    rules:
      - alert: BackupFailed
        expr: time() - backup_last_success_timestamp > 86400
        for: 1h
        labels:
          severity: critical
          component: backup
        annotations:
          summary: "Backup has not completed successfully"
          description: "Last successful backup was {{ $value }} seconds ago"
      
      - alert: BackupSizeTooLarge
        expr: backup_size_bytes > 100e9
        for: 5m
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "Backup size is large"
          description: "Backup size is {{ $value | humanize }}B"

  # ============================================================================
  # SSL Certificate Alerts
  # ============================================================================
  - name: ssl_alerts
    interval: 12h
    rules:
      - alert: SSLCertificateExpiringSoon
        expr: (ssl_certificate_expiry_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
          component: ssl
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.domain }} expires in {{ $value }} days"
      
      - alert: SSLCertificateExpired
        expr: ssl_certificate_expiry_seconds - time() < 0
        for: 1m
        labels:
          severity: critical
          component: ssl
        annotations:
          summary: "SSL certificate has expired"
          description: "SSL certificate for {{ $labels.domain }} has expired"
